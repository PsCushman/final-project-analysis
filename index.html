<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="static/css/style.css">    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Alfa+Slab+One&family=Special+Elite&display=swap" rel="stylesheet">

</head>
<body>
    <div class="row">

        <div class="col-sm-9 col-md-7 main">
            <div class="jumbotron">
                <h1 class="display-4">Decoding the Diamond Dynamics:</h1>
                <p class="first-paragraph">Baseball Analysis for a New Game</p>
                <p class="second-paragraph">Predicting wOBA, SLG, BABIP, and wRC+ After Recent Rule Changes</p>
            </div>
            
            <section>
                <h2>Death to the Shift!</h2>
                <p>The Modern Era of baseball (2000s-2020s) has seen an increased emphasis on advanced statistics and analytics that has revolutionized player evaluation and strategic decision-making.</p>
                <p>Teams leveraged data to evaluate players more precisely, influencing strategic decisions. Pitchers excelled as teams shifted to power pitching as well as optimizing matchups with pitching changes and defensive shifts. </p>
                <p>To combat these changes, hitters began emphasising launch angles and power hitting, leading to fewer balls in play.</p>
                <p>To maintain pace of play and increase excitment, this season the MLB implemented a pitch clock, limited defensive shifts, and a instituted a universal DH, along a few changes to increase base stealing attempts.</p>
                <p>All together, these changes were expected to increase pace of play, increase balls in play, and over all, just make the game more exciting.</p>
                <div class="centered-section">
                <button class="link-button" onclick="window.open('https://www.mlb.com/news/mlb-2023-rule-changes-pitch-timer-larger-bases-shifts/', '_blank')">Rule Changes</button>
                </div>
            </section>

            <section>
                <h2>Project Overview</h2>
                <p>Focusing on batters with more than 150 plate appearences in 2018-2023 (with 2020 excluded), I attempted to uncover which features might predict improved or declining player performance.</p> 
                <p>By employing machine learning algorithms on historical player data, the project aims to construct predictive models that classify wether a player would improve in 4 key metrics:</p>
                    <br>
                    <div class="centered-section">
                        <p><strong>Overall Hitting Performance => wOBA</strong></p>
                        <p><strong>Power => SLG</strong></p>
                        <p><strong>Average on In-Play Balls => BABIP</strong></p>
                        <p><strong>Run Creation => wRC+</strong></p>
                    </div>
            </section>

            <br>
            <section>
                <h2>Clean Up (Hitter)</h2>
                <p>Like most data analysis projects, this one started with hours of ETL and data cleaning. I started with <a href="https://github.com/jldbc/pybaseball, '_blank'">Pybaseball</a>, a module that could be easily installed and which scrapes the website <a href="https://www.fangraphs.com/">Fangraphs</a> for historical and present baseball data.</p>
                <p>However, the pybaseball module had issues halfway through the ETL process, so I turned to Frangraphs directly to export 2023 data to add to my pervious data.</p>
                <pre><code class="language-python">
            relevant_years = [2018, 2019, 2021, 2022]
            filtered_batting_data = batting_data[batting_data['year'].isin(relevant_years)]
            
            
            woba_2023 = batting_23[['player_id', 'woba']].rename(columns={'woba': 'woba_2023'})
            batting_grouped_data = filtered_batting_data .groupby(['player_id', 'name']).mean()
            
            # Merge player names with the result DataFrame
            players_batting_output = pd.merge(
                batting_grouped_data,
                woba_2023,
                on=['player_id'])
                    </code></pre>
                <p>I used two different sets of data. One had over 250 features or columns with advanced statcast metrics. The other included 12 colums and inclued more traditional metrics.</p>
                <p>I ended up choosing the dataset with the most features. I had run some intial models with the less features and was not getting great results.</p>
                <p>In addition, because I was only returning 256 batters that had played a min of two years from 2018-2022 with a minimun of 150 PA and also had a min of 150 PA this year, I didn't have a ton of data to train my models on.</p>
                <p>Even after selecting this dataset, I decided to try pairing down my data using correlation plots, but my inital models were not as sucessful. In all honesty, at the cleaning portion, most of my models were not great.</p>
                <br>
                <br>
                <div style="text-align: center;">
                    <p><strong>Feature Correlation Plot</strong></p>
                    <br>
                    <img src="static/images/too_many_features.png" alt="Correlation Plots" style="width: 50%;">
                </div>
                <p>In the end, more features reliably returned more accurate models and so, I kept as many columns as I could (I did have to drop some with NAN values).</p>
                <p>Lastly, I chose to zscore and differences between the the averages for the prior years. This is because, the rule changes were meant to increase offensiive outcomes for all batters.</p>
                <p>The point of this exersise is to see if we could predict who was benifiting the most from the rule changes the most. Therefore, I compared the zscores so that we could see who improved or delined when compared to their peers.</p>
            </section>

            <section>
                <h2>Making the Model</h2>
                <p>I tried everything. Regressions, Nueral Networks, Oversampling, Logistic regressions, Tuners and Crooners.</p>
                <p>I got a lot of help from <a href="https://johnpette.medium.com/" target="_blank">John Pette's predictions</a> and the machine learning methods contained there in. XGBoost and SMOTE where new to me and it has some great explianations and links to even better explainations.</p>
                <p>In the end, once I wrote out the function and could quickly load and rerun models, I, through trail and error, got something to work with.</p>
                <br>
                <p style="text-align: center;"><strong>XGBoost Classifier with SMOTE Over Sampling</strong></p>
                <pre><code class="language-python">
        def train_and_evaluate_classifacation_model(df, target_column):
            # Drop rows with missing values
            df.dropna(axis=1, inplace=True)
            
            # Convert the target column into binary classes (0 or 1)
            df[target_column] = df[target_column].apply(lambda x: 1 if x > 0 else 0)
            
            # Split data into features (X) and target (y)
            X = df.drop(target_column, axis=1)
            y = df[target_column]
            
            # Split data into training and testing sets
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Standardize features using StandardScaler
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Define cross-validation strategy
            kf = KFold(n_splits=5, shuffle=True, random_state=42)
            
            # Build the pipeline
            imb_pipeline = Pipeline([
                ('smote', SMOTE(random_state=42)),
                ('xgbclassifier', xgb.XGBClassifier(
                    max_depth=5,
                    learning_rate=0.01,
                    n_estimators=600,
                    subsample=0.5,
                    colsample_bytree=0.25,
                    objective='binary:logistic',
                    random_state=42
                ))
            ])
            
            # Train the pipeline on the training data
            imb_pipeline.fit(X_train_scaled, y_train)
            
            # Make predictions on the test set
            y_pred = imb_pipeline.predict(X_test_scaled)
            
            # Create the SHAP explainer
            final_estimator = imb_pipeline.named_steps['xgbclassifier']
            explainer = shap.Explainer(final_estimator, X_train_scaled)
            
            # Calculate SHAP values
            shap_values = explainer(X_test_scaled, check_additivity=False)
            
            # Calculate evaluation metrics
            balanced_recall = balanced_accuracy_score(y_test, y_pred)
            accuracy = accuracy_score(y_test, y_pred)
            
            return {
                "accuracy": accuracy,
                "balanced_recall": balanced_recall,
                "shap_values": shap_values,
                "X_test_scaled": X_test_scaled,
                "X_train": X_train,
                "X_train_scaled": X_train_scaled,
                "y_pred": y_pred,
                "y_test": y_test,
                "y_train": y_train,
                "X_test": X_test
        }
            </code></pre>
                <p>Here, I defined a function that takes a dataset and a target column name as inputs and trains an XGBoost classification model with SMOTE oversampling.</p> 
                <p>The function preprocesses the data by dropping missing values, converting the target into binary classes, and splitting it into training and testing sets. 
                <p>It standardizes features and builds a pipeline with SMOTE and XGBoost classifier components. The model is trained, predictions are made, and SHAP values are computed so that we can examine feature importance.</p>
            <div class="centered-section">
                <button class="link-button" onclick="window.open('https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/', '_blank')">What is SMOTE?</button>
            </div>
            </section>

            <section>
                <h2>Above the Mendoza Line: Predicting wOBA Improvement</h2>
                <p>So here we go! After cleaning the data and setting things up for success, I ran the model. I chose wOBA as my first model, because it mesaures overall hitting production.</p>
                <p>I called the function and ran it.</p>
                <pre><code class="language-python">
                #Run the model with all the features.
                result_woba = train_and_evaluate_classifacation_model(batter_woba_df, 'zscore_difference_woba')

                Accuracy: 0.65
                Recall: 0.65    
                </code></pre>
                <p>In all seriousness, it wasn't perfect, but I was actually happy enough with it considering all the things that could go wrong or right for a batter in a given season.</p>
                <p>Now, what I was most interested in is the feature proformance or, what elements were the algorithms using most to make this perdiction. Because there are so many columns, the feature importanc is relativeley low for each, even for the most important.</p>
                <p>However, the top few were a lot more imprtant than the rest.</p>
                <p>I used the SHAP module to calculate and plot the importance.</p>
                <div style="text-align: center;">
                    <p><strong>SHAP Violin Plot for wOBA</strong></p>
                <img src="static/images/shap_summary_plot_woba copy.png" alt="Shap wOBA">
                </div>
                <p>Turns out, to predict over hitting preformance, you can look to OPS, Hard Hit%, and BB/K ratio as incicators to predict overall hitting performance under the new rules.</p>
                <p>In fact, when I took the top five features and ran the model with only those, I got the same Accuracy and Recall.</p> 
                <pre><code class="language-python">
                #Run the model with top 5 most important features.
                result_woba_less_c = train_and_evaluate_classifacation_model(batter_woba_df, 'zscore_difference_woba')
    
                        Accuracy: 0.65
                        Recall: 0.65    
                    </code></pre>
                <p>This didn't work with every model, but once I knew the most imporantant features for predicting wOBA, I decreased the features without effecting perfomance.</p>
            </section>

            <section>
                <h2>Handedness</h2>
                <p>When going over the data, I doubled checked to see if BABIP has predictably increased for left-handed batters as this is the group most affected by defensive shifts. Looking chart below, you can see that as analytics and denfinsive shifts gained more importance after the turn of the century, lefties were disprontionally affected.</p>
                <div style="text-align: center;">
                    <p><strong>Batting Against the Shift before 2023 </strong></p>
                <img src="static/images/Shifting-Lefties.png" alt="Shifting", style="width: 50%;">
                </div>
                <p>I figured adding handedness in my models would massiveley increase my accuracy. So much in fact, that I considered not doing it because I was going to have to do some ETL to get to it.</p>
                <p>And I thought, handedness might be too closely linked with the outcome variable, to the extent that it might introduce redundancy or overfitting in a predictive model. But, I did it anyway so I could see some models in the 90s.</p>
                <p>Fangraphs does not include handedness, perhaps becuase they do include splits batting AVG, OPS, Etc. agaist RHP v LHP(more on that later). So I had to find a database with the handedness included, which I did at <a href="https://baseballsavant.mlb.com//">Baseball Savant.</a></p>
                <p>After an annoying merge and one HAWWWT coding, I ran my models and they all turned out worse by at least two percentage points, except the one we might expect: BABIP.</p>
                <br>
                <p style="text-align: center;"><strong>BABIP Model Performance without Handedness</strong></p>
                <pre><code class="language-python">
        result_babip = train_and_evaluate_classifacation_model(batter_babip_df, 'zscore_difference_babip')

                Accuracy: 0.73
                Recall: 0.73
                </code></pre>
                <p style="text-align: center;"><strong>BABIP Model Performance with Handedness</strong></p>
                <pre><code class="language-python">
        result_babip_w_bat_side = train_and_evaluate_classifacation_model(batter_w_bat_side, 'zscore_difference_babip')

                Accuracy: 0.75
                Recall: 0.75
                </code></pre>
                <p>However, for the rest of the models, the performance dip a point or two. Because handedness seemed such a stark distinction, I thought it would really improve all models.</p>
                <p>Yet, including so many features from the start, may have rendered handedness uncessary. The model mostly likely handeled it inevertley through splits and other features. Adding bat-side, was most likley redudant and may have even contadicted some elements that helped make the pervious predictions.</p>
                <p>Out of 59% of lefties increased their zscores and only 40% of righties. By far the biggest difference.</p>
                <p>In contrast, for wOBA, 52% of lefties increased and 41% of righties. For SLG, it was 49% and 44%. And lastly, for wRC+ it was 52% and 43%.</p>
                <p>So while it is true that lefties have benifited most from these new rules, it does not mean that handedness is necessarily the best predictor when we have all the other stats.</p>
                <p>Click the button for an article that describes the possibilty that bat side may not be the best predictor of success with the rule changes for all players.</p>
                <div class="centered-section">
                    <button class="link-button" onclick="window.open('https://www.mlb.com/news/shift-rule-effect-on-lefty-hitters/', '_blank')">Will Shift Ban Help These 6 Hitters? Not So Fast...</button>
                </div>
                </section> 
            </section>

            <section>
                <h2>Barrel Up: Results</h2>
                <p>While I focues on wOBA to start, my best model was SLG at 77% accurancy, with BABIP, wOBA, and wRC+ different levels of success.</p>
                <pre><code class="language-python">
                result_slg = train_and_evaluate_classifacation_model(batter_slg_df, 'zscore_difference_slg')
            
                print(f"Accuracy: {result_slg ['accuracy']:.2f}")
                print(f"Recall: {result_slg ['balanced_recall']:.2f}")
                            
                            Accuracy: 0.77
                            Recall: 0.77
                </code></pre>
                <p>BABIP (with handedness) topped out at 75%. wRC+ at 67% and as we saw before, wOBA couldn't crack 65%.</p>
                <p>Now let's check out the SHAP plots to determine feature importance.</p>
                <div style="text-align: center;">
                    <p><strong>SHAP Violin Plot for SLG</strong></p>
                <img src="static/images/shap_slg_class copy.png" alt="Shap SLG">
                </div>
                <p>This one had the craziest stats.</p>
                <div style="text-align: center;">
                    <p><strong>SHAP Violin Plot for BABIP</strong></p>
                <img src="static/images/shap_babip_class_handy copy.png" alt="Shap BABIP">
                </div>
                <p>Shap and feature importance for each one.</p>
                <div style="text-align: center;">
                    <p><strong>SHAP Violin Plot for wRC+</strong></p>
                <img src="static/images/shap_wrc_class copy.png" alt="Shap wRC+">
                </div>
                <p>Shap and feature importance for each one.</p>
                <p>I made a player dashboard where we can imput a players name and it reurns the zscore differences for all the target variables.</p>
                <p>And, more importantly whether we got the prediction right.</p>
                <button class="link-button" onclick="window.open('http://127.0.0.1:8050', '_blank')">Prediction Dashboard</button>
            </section>

            <section>
                <h2>Rounding Third: Where to Go from Here</h2>
                <p>Obviously, home right. No seriously, let's talk about the usefulness of this project.</p>
                <p>I'm sure it's out there, but I wish I could have found a column tuner. Not PCA because I needed readability to understand the model. But a tunerer that could work through each combo of features so that we could find the ideal amount and figuration.</p>
                <p>First, I'd like to talk about Tim Anderson of my hometown Chicago White Sox.</p>
                <p>He's had a horrible year. But</p>
                <p> Our queries found that Alice Fong Young's School did the best and the marked on our story map.<p>
                <p>In case you are wondering, Sts. Peter and Paul School did the worst.</p>
            </section>

</body>
</html>