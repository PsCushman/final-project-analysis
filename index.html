<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="static/css/style.css">    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Alfa+Slab+One&family=Special+Elite&display=swap" rel="stylesheet">

</head>
<body>
    <div class="row">

        <div class="col-sm-9 col-md-7 main">
            <div class="jumbotron">
                <h1 class="display-4">Decoding the Diamond Dynamics:</h1>
                <p class="first-paragraph">Baseball Analysis for a New Game</p>
                <p class="second-paragraph">Predicting wOBA, SLG, BABIP, and wRC+ After Recent Rule Changes</p>
            </div>
            
            <section>
                <h2>Death to the Shift!</h2>
                <p>The Modern Era of baseball (2000s-2020s) has seen an increased emphasis on advanced statistics and analytics that has revolutionized player evaluation and strategic decision-making.</p>
                <p>Teams leveraged data to evaluate players more precisely, influencing strategic decisions. Pitchers excelled as teams shifted to power pitching as well as optimizing matchups with pitching changes and defensive shifts. </p>
                <p>To combat these changes, hitters began emphasising launch angles and power hitting, leading to fewer balls in play.</p>
                <p>To maintain pace of play and increase excitment, this season the MLB implemented a pitch clock, limited defensive shifts, and a instituted a universal DH, along a few changes to increase base stealing attempts.</p>
                <p>All together, these changes were expected to increase pace of play, increase balls in play, and over all, just make the game more exciting.</p>
                <div class="centered-section">
                <button class="link-button" onclick="window.open('https://www.mlb.com/news/mlb-2023-rule-changes-pitch-timer-larger-bases-shifts/', '_blank')">Rule Changes</button>
                </div>
            </section>

            <section>
                <h2>Project Overview</h2>
                <p>Focusing on batters with more than 150 plate appearences in 2018-2023 (with 2020 excluded), I attempted to uncover which features might predict improved or declining player performance.</p> 
                <p>By employing machine learning algorithms on historical player data, the project aims to construct predictive models that classify wether a player would improve in 4 key metrics:</p>
                    <br>
                    <div class="centered-section">
                        <p><strong>Overall Hitting Performance => wOBA</strong></p>
                        <p><strong>Power => SLG</strong></p>
                        <p><strong>Average on In-Play Balls => BABIP</strong></p>
                        <p><strong>Run Creation => wRC+</strong></p>
                    </div>
            </section>

            <br>
            <section>
                <h2>Clean Up (Hitter)</h2>
                <p>Like most data analysis projects, this one started with hours of ETL and data cleaning. I started with <a href="https://github.com/jldbc/pybaseball, '_blank'">Pybaseball</a>, a module that could be easily installed and which scrapes the website <a href="https://www.fangraphs.com/">Fangraphs</a> for historical and present baseball data.</p>
                <p>However, the pybaseball module had issues halfway through the ETL process, so I turned to Frangraphs directly to export 2023 data to add to my pervious data.</p>
                <pre><code class="language-python">
            relevant_years = [2018, 2019, 2021, 2022]
            filtered_batting_data = batting_data[batting_data['year'].isin(relevant_years)]
            
            
            woba_2023 = batting_23[['player_id', 'woba']].rename(columns={'woba': 'woba_2023'})
            batting_grouped_data = filtered_batting_data .groupby(['player_id', 'name']).mean()
            
            # Merge player names with the result DataFrame
            players_batting_output = pd.merge(
                batting_grouped_data,
                woba_2023,
                on=['player_id'])
                    </code></pre>
                <p>I used two different sets of data. One had over 250 features or columns with advanced statcast metrics. The other included 12 colums and inclued more traditional metrics.</p>
                <p>I ended up choosing the dataset with the most features. I had run some intial models with the less features and was not getting great results.</p>
                <p>In addition, because I was only returning 256 batters that had played a min of two years from 2018-2022 with a minimun of 150 PA and also had a min of 150 PA this year, I didn't have a ton of data to train my models on.</p>
                <p>Even after selecting this dataset, I decided to try pairing down my data using correlation plots, but my inital models were not as sucessful. In all honesty, at the cleaning portion, most of my models were not great.</p>
                <br>
                <br>
                <div style="text-align: center;">
                    <p><strong>Feature Correlation Plot</strong></p>
                    <br>
                    <img src="static/images/too_many_features.png" alt="Correlation Plots" style="width: 50%;">
                </div>
                <p>In the end, more features reliably returned more accurate models and so, I kept as many columns as I could (I did have to drop some with NAN values).</p>
                <p>Lastly, I chose to Z-score and differences between the the averages for the prior years. This is because, the rule changes were meant to increase offensiive outcomes for all batters.</p>
                <p>The point of this exersise is to see if we could predict who was benifiting the most from the rule changes and why. Therefore, I compared the Z-scores so that we could see who improved or delined when compared to their peers.</p>
            </section>

            <section>
                <h2>Making the Model</h2>
                <p>I tried everything. Regressions, Nueral Networks, Oversampling, Logistic Regressions, Tuners and Crooners.</p>
                <p>I got a lot of help from <a href="https://johnpette.medium.com/" target="_blank">John Pette's predictions</a> and the machine learning methods contained there in. XGBoost and SMOTE where new to me and it has some great explianations and links to even better explainations.</p>
                <p>In the end, once I wrote out the function and could quickly load and rerun models, I, through trail and error, got something to work with.</p>
                <br>
                <p style="text-align: center;"><strong>XGBoost Classifier with SMOTE Over Sampling</strong></p>
                <pre><code class="language-python">
        def train_and_evaluate_classifacation_model(df, target_column):
            # Drop rows with missing values
            df.dropna(axis=1, inplace=True)
            
            # Convert the target column into binary classes (0 or 1)
            df[target_column] = df[target_column].apply(lambda x: 1 if x > 0 else 0)
            
            # Split data into features (X) and target (y)
            X = df.drop(target_column, axis=1)
            y = df[target_column]
            
            # Split data into training and testing sets
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Standardize features using StandardScaler
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Define cross-validation strategy
            kf = KFold(n_splits=5, shuffle=True, random_state=42)
            
            # Build the pipeline
            imb_pipeline = Pipeline([
                ('smote', SMOTE(random_state=42)),
                ('xgbclassifier', xgb.XGBClassifier(
                    max_depth=5,
                    learning_rate=0.01,
                    n_estimators=600,
                    subsample=0.5,
                    colsample_bytree=0.25,
                    objective='binary:logistic',
                    random_state=42
                ))
            ])
            
            # Train the pipeline on the training data
            imb_pipeline.fit(X_train_scaled, y_train)
            
            # Make predictions on the test set
            y_pred = imb_pipeline.predict(X_test_scaled)
            
            # Create the SHAP explainer
            final_estimator = imb_pipeline.named_steps['xgbclassifier']
            explainer = shap.Explainer(final_estimator, X_train_scaled)
            
            # Calculate SHAP values
            shap_values = explainer(X_test_scaled, check_additivity=False)
            
            # Calculate evaluation metrics
            balanced_recall = balanced_accuracy_score(y_test, y_pred)
            accuracy = accuracy_score(y_test, y_pred)
            
            return {
                "accuracy": accuracy,
                "balanced_recall": balanced_recall,
                "shap_values": shap_values,
                "X_test_scaled": X_test_scaled,
                "X_train": X_train,
                "X_train_scaled": X_train_scaled,
                "y_pred": y_pred,
                "y_test": y_test,
                "y_train": y_train,
                "X_test": X_test
        }
            </code></pre>
                <p>Here, I defined a function that takes a dataset and a target column name as inputs and trains an XGBoost classification model with SMOTE oversampling.</p> 
                <p>The function preprocesses the data by dropping missing values, converting the target into binary classes, and splitting it into training and testing sets. 
                <p>It standardizes features and builds a pipeline with SMOTE and XGBoost classifier components. The model is trained, predictions are made, and SHAP values are computed so that we can examine feature importance.</p>
            <div class="centered-section">
                <button class="link-button" onclick="window.open('https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/', '_blank')">What is SMOTE?</button>
            </div>
            </section>

            <section>
                <h2>Above the Mendoza Line: Predicting wOBA Improvement</h2>
                <p>So here we go! After cleaning the data and setting things up for success, I ran the model. I chose wOBA as my first model, because it mesaures overall hitting production.</p>
                <p>I called the function and ran it.</p>
                <pre><code class="language-python">
                #Run the model with all the features.
                result_woba = train_and_evaluate_classifacation_model(batter_woba_df, 'zscore_difference_woba')

                Accuracy: 0.65
                Recall: 0.65    
                </code></pre>
                <p>In all seriousness, it wasn't perfect, but I was actually happy enough with it considering all the things that could go wrong or right for a batter in a given season.</p>
                <p>Now, what I was most interested in is the feature proformance or, what elements were the algorithms using most to make this perdiction. Because there are so many columns, the feature importanc is relativeley low for each, even for the most important.</p>
                <p>However, the top few were a lot more imprtant than the rest.</p>
                <p>I used the SHAP module to calculate and plot the importance.</p>
                <div style="text-align: center;">
                    <p><strong>SHAP Violin Plot for wOBA</strong></p>
                <img src="static/images/shap_summary_plot_woba copy.png" alt="Shap wOBA">
                </div>
                <p>Turns out, to predict over hitting preformance, you can look to OPS, Hard Hit%, and BB/K ratio as incicators to predict overall hitting performance under the new rules.</p>
                <p>In fact, when I took the top five features and ran the model with only those, I got the same Accuracy and Recall.</p> 
                <pre><code class="language-python">
                #Run the model with top 5 most important features.
                result_woba_less_c = train_and_evaluate_classifacation_model(batter_woba_df, 'zscore_difference_woba')
    
                        Accuracy: 0.65
                        Recall: 0.65    
                    </code></pre>
                <p>This didn't work with quite as well every model, but now I knew the most imporantant features for predicting wOBA, even when exculding all the other statistics, could do some pretty reliable predicting.</p>
                <p>And, it gave me confidence that the feature importance of other models were turly important and could be used to understand why certain players were more sucessful after the rule changes.</p>
            </section>

            <section>
                <h2>Handedness</h2>
                <p>When going over the data, I checked to see if BABIP has predictably increased for left-handed batters as this is the group most affected by defensive shifts. Looking at the chart below, you can see that as analytics and denfinsive shifts gained more importance after the turn of the century, lefties were disprontionally affected.</p>
                <div class="centered-section">
                    <p><strong>Batting Against the Shift before 2023 </strong></p>
                <img src="static/images/Shifting-Lefties.png" alt="Shifting", style="width: 50%;">
            </div>
            <br>
            <div class="centered-section">
                <button class="link-button" onclick="window.open('https://www.si.com/more-sports/2011/05/10/righty-overshift/', '_blank')">Why are teams reluctant to shift against righthanded batters?</button>
                </div>
                <br>
                <p>I figured adding handedness in my models would massiveley increase my accuracy. So much in fact, that I considered not doing it because I was going to have to do some ETL to get to it.</p>
                <p>And I thought, handedness might be too closely linked with the outcome variable, to the extent that it might introduce redundancy or overfitting in a predictive model. But, I did it anyway so I could see some models in the 90s.</p>
                <p>Fangraphs does not include handedness, perhaps becuase they do include splits batting AVG, OPS, Etc. agaist RHP v LHP(more on that later). So I had to find a database with the handedness included, which I did at <a href="https://baseballsavant.mlb.com//">Baseball Savant.</a></p>
                <p>After an annoying merge and one HAWWWT coding, I ran my models and they all turned out worse by at least two percentage points, except the one we might expect: BABIP.</p>
                <br>
                <p style="text-align: center;"><strong>BABIP Model Performance without Handedness</strong></p>
                <pre><code class="language-python">
        result_babip = train_and_evaluate_classifacation_model(batter_babip_df, 'zscore_difference_babip')

                Accuracy: 0.73
                Recall: 0.73
                </code></pre>
                <p style="text-align: center;"><strong>BABIP Model Performance with Handedness</strong></p>
                <pre><code class="language-python">
        result_babip_w_bat_side = train_and_evaluate_classifacation_model(batter_w_bat_side, 'zscore_difference_babip')

                Accuracy: 0.75
                Recall: 0.75
                </code></pre>
                <p>However, for the rest of the models, the performance dip a point or two. Because handedness seemed such a stark distinction, I thought it would really improve all models.</p>
                <p>Yet, including so many features from the start, may have rendered handedness uncessary. The model mostly likely handeled it inevertley through splits and other features. Adding bat-side, was most likley redudant and may have even contadicted some elements that helped make the pervious predictions.</p>
                <p>Out of 59% of lefties increased their Z-scores and only 40% of righties. By far the biggest difference.</p>
                <p>In contrast, for wOBA, 52% of lefties increased and 41% of righties. For SLG, it was 49% and 44%. And lastly, for wRC+ it was 52% and 43%.</p>
                <p>So while it is true that lefties have benifited most from these new rules, it does not mean that handedness is necessarily the best predictor when we have all the other stats.</p>
                <p>Click the button for an article that describes the possibilty that bat side may not be the best predictor of success with the rule changes for all players.</p>
                <div class="centered-section">
                    <button class="link-button" onclick="window.open('https://www.mlb.com/news/shift-rule-effect-on-lefty-hitters/', '_blank')">Will Shift Ban Help These 6 Hitters? Not So Fast...</button>
                </div>
                </section> 
            </section>

            <section>
                <h2>Barrel Up: Results</h2>
                <p>While I focued on wOBA to start, my best model was SLG at 77% accurancy, with BABIP, wOBA, and wRC+ different levels of success.</p>
                <pre><code class="language-python">
                result_slg = train_and_evaluate_classifacation_model(batter_slg_df, 'zscore_difference_slg')
            
                print(f"Accuracy: {result_slg ['accuracy']:.2f}")
                print(f"Recall: {result_slg ['balanced_recall']:.2f}")
                            
                            Accuracy: 0.77
                            Recall: 0.77
                </code></pre>
                <p>BABIP (with handedness) topped out at 75%. wRC+ at 67% and as we saw before, wOBA couldn't crack 65%.</p>
                <p>Now let's check out the SHAP plots to determine feature importance.</p>
                <div style="text-align: center;">
                    <p><strong>SHAP Violin Plot for SLG</strong></p>
                <img src="static/images/shap_slg_class copy.png" alt="Shap SLG">
                </div>
                <p>This one is just kinda wild to me. Many of SLG's most important features such as SL-Z(sc), which measures a batter's ability to make contact with sliders thrown within the strike zone, providesd insights into a batter's performance against offspeed piches and their ability to recognize and react to them in the zone. </p>
                <p>More analyis is needed to determine just why that would be. But they are negativley correlated meaning that if you were bad at hitting offspeed before, it was actually a prediction that you would have more power this year.</p>
                <p>This could be an indication of less intimating "stuff" as the pitch clock has sped up pichers, leading to in those player that may have struggled before.</p>
                <div style="text-align: center;">
                    <p><strong>SHAP Violin Plot for BABIP</strong></p>
                <img src="static/images/shap_babip_class_handy copy.png" alt="Shap BABIP">
                </div>
                <p>Intestingly, bat_side is not even on the SHAP plot. What is cool, and makes total sense when you think about it, is that the Avg BABIP from years past was most important in predicting 2023's BABIP.</p>
                <p>However, it was inversley connected. It had high feature importance in the negative direction. Meaning if you had lower BABIP before (like many lefties), you were likley to see a more positive Z-score difference.</p>
                <div style="text-align: center;">
                    <p><strong>SHAP Violin Plot for wRC+</strong></p>
                <img src="static/images/shap_wrc_class copy.png" alt="Shap wRC+">
                </div>
                <p>wRC+ plus may be the least interesting. The model essetially predicted that is you had trouble before creating runs, the was a likleyhood that you would get better under the new rules when compared to others that may have all peak. Essentially predicting regression to the mean.</p>
                <br>
                <p>To display the results, I player dashboard where a user can imput a players name and it reurns the Z-score differences for all the target variables.</p>
                <p>And, more importantly whether we got the prediction right.</p>
                <div class="centered-section">
                <button class="link-button" onclick="window.open('http://127.0.0.1:8050', '_blank')">Prediction Dashboard</button>
                </div>
            </section>

            <section>
                <h2>Rounding Third: Where to Go from Here</h2>
                <p><strong>Home! Obviously. Right?</strong> </p>
                <br>
                <p>Understanding how modern rule changes have impacted player performance, was the aim of this project. It delved into the intricate interplay between baseball analytics, statistical modeling, and machine learning. Through the predictive power of various player metrics, we got to shed  some light on which factors could foreseeably determine success under the evolving landscape of the sport.</p>
                <p>The exploration into the relationship between advanced statistics and player performance yielded both insights and challenges. The integration of machine learning techniques, such as XGBoost with SMOTE oversampling, showcased the potential of harnessing data to uncover underlying trends and patterns. While the models achieved reasonable accuracy in predicting performance metrics such as SLG and BABIP, it's important to note that no model captured the full complexity of human performance within the ever-changing context of a baseball season.</p>
                <p>As with any endeavor in data analysis, this project has raised more questions than it has answered. The results indicate that certain features play a crucial role in predicting player improvement or regression, yet the "why" remains open for interpretation. Deeper analysis into the specific impact of rule changes, such as the effect of the pitch clock on batter-pitcher interactions could provide a richer understanding of the observed trends.</p>
                <p>Future explorations could involve refining the models by incorporating more granular data, considering player trajectories over multiple seasons, and examining the the use of some sort of feature tuner. Additionally, investigating the implications of these findings for player development, team strategies, and the game as a whole could offer valuable insights into the broader implications of the rule changes.</p>
                <p>Ultimately, this project was fun and illustrated the intersection of baseball, data and technology. More than any sport, it lends itself to analysis and it continues to be a fertile ground for exploration.</p>
            </section>
            

</body>
</html>