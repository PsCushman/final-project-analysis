<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="static/css/style.css">    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Alfa+Slab+One&display=swap" rel="stylesheet">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Special+Elite&display=swap" rel="stylesheet">

</head>
<body>
    <div class="row">

        <div class="col-sm-9 col-md-7 main">
            <div class="jumbotron">
                <h1 class="display-4">Changes Rule!</h1>
                <p class="first-paragraph">Making predictions on:</p>
                <p class="second-paragraph">wOBA, SLG, BABIP, and wRC+.</p>
            </div>
            
            <section>
                <h2>Death to the Shift!</h2>
                <p>The Modern Era of baseball (2000s-2020s) has seen an increased emphasis on advanced statistics and analytics that has revolutionized player evaluation and strategic decision-making.</p>
                <p>Teams leveraged data to evaluate players more precisely, influencing strategic decisions. Pitchers excelled as teams shifted to power pitching, optimizing matchups with pitching changes and defensive shifts. </p>
                <p>To combat these changes, batters began emphasising on launch angles and power hitting led to fewer balls in play.</p>
                <p>To maintain pace of play and increase excitment, this season the MLB implemented a pitch clock, limited defensive shifts, and a instituted a universal DH, along a few changes to increase base stealing.</p>
                <p>All together, these changes were expected to increase pace of play, increase balls in play, and over all, just make the game more exciting.</p>
                <div class="centered-section">
                <button class="link-button" onclick="window.open('https://www.mlb.com/news/mlb-2023-rule-changes-pitch-timer-larger-bases-shifts/', '_blank')">Rule Changes</button>
                </div>
            </section>

            <section>
                <h2>Project Overview</h2>
                <p>Focusing on batters with more than 150 plate appearences in 2018-2023 (with 2020 excluded), I attempted to uncover which features might predict improved or declining player performance.</p> 
                <p>By employing machine learning algorithms on historical player data, the project aims to construct predictive models that classify wether a player would improve in 4 key metrics:</p>
                    <br>
                    <div class="centered-section">
                        <p><strong>Overall Hitting Performance => wOBA</strong></p>
                        <p><strong>Power => SLG</strong></p>
                        <p><strong>Average on In-Play Balls => BABIP</strong></p>
                        <p><strong>Run Creation => wRC+</strong></p>
                    </div>
            </section>

            <br>
            <section>
                <h2>Clean Up (Hitter)</h2>
                <p>Like most data analysis projects, this one started with hours of ETL and data cleaning. I started with <a href="https://github.com/jldbc/pybaseball">Pybaseball</a>, a module that could be easily installed and which scrapes the website <a href="https://www.fangraphs.com/">Fangraphs</a> for historical and present baseball data.</p>
                <p>However, the pybaseball module had issues halfway through the ETL process, so I turned to Frangraphs directly to export 2023 data to add to my pervious data.</p>
                <pre><code class="language-python">
            relevant_years = [2018, 2019, 2021, 2022]
            filtered_batting_data = batting_data[batting_data['year'].isin(relevant_years)]
            
            
            woba_2023 = batting_23[['player_id', 'woba']].rename(columns={'woba': 'woba_2023'})
            batting_grouped_data = filtered_batting_data .groupby(['player_id', 'name']).mean()
            
            # Merge player names with the result DataFrame
            players_batting_output = pd.merge(
                batting_grouped_data,
                woba_2023,
                on=['player_id'])
                    </code></pre>
                <p>I used two different sets of data. One had over 250 features or columns with advanced statcast metrics. The other included 12 colums and inclued more traditional metrics.</p>
                <p>I ended up choosing the dataset with the most features. I had run some intial models with the less features and was not getting great results.</p>
                <p>In addition, because I was only returning 256 batters that had played a min of two years from 2018-2022 with a minimun of 150 PA and also had a min of 150 PA this year, I didn't have a ton of data to train my models on.</p>
                <p>Even after selecting this dataset, I decided to try pairing down my data using correlation plots, but my inital models were not as sucessful. In all honesty, at the cleaning portion, most of my models were not great.</p>
                <br>
                <br>
                <div style="text-align: center;">
                    <p><strong>Feature Correlation Plot</strong></p>
                    <br>
                    <img src="static/images/too_many_features.png" alt="Correlation Plots" style="width: 50%;">
                </div>
                <p>In the end, more features reliably returned more accurate models and so, I kept as many columns as I could (I did have to drop some with NAN values).</p>
                <p>Lastly, I chose to zscore and differences between the the averages for the prior years. This is because, the rule changes were meant to increase offensiive outcomes for all batters.</p>
                <p>The point of this exersise is to see if we could predict who was benifiting the most from the rule changes the most. Therefore, I compared the zscores so that we could see who improved or delined when compared to their peers.</p>
            </section>

            <section>
                <h2>Making the Model</h2>
                <p>I tried everything. Regressions, Nueral Networks, Oversampling, Logistic regressions, Tuners and Crooners.</p>
                <p>I got a lot of help from....</p>
                <p>In the end, once I wrote out the function and could quickly load and rerun models, I, through trail and error, got something to work with.</p>
                <br>
                <p style="text-align: center;"><strong>XGBoost Classifier with SMOTE Over Sampling</strong></p>
                <pre><code class="language-python">
        def train_and_evaluate_classifacation_model(df, target_column):
            # Drop rows with missing values
            df.dropna(axis=1, inplace=True)
            
            # Convert the target column into binary classes (0 or 1)
            df[target_column] = df[target_column].apply(lambda x: 1 if x > 0 else 0)
            
            # Split data into features (X) and target (y)
            X = df.drop(target_column, axis=1)
            y = df[target_column]
            
            # Split data into training and testing sets
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Standardize features using StandardScaler
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Define cross-validation strategy
            kf = KFold(n_splits=5, shuffle=True, random_state=42)
            
            # Build the pipeline
            imb_pipeline = Pipeline([
                ('smote', SMOTE(random_state=42)),
                ('xgbclassifier', xgb.XGBClassifier(
                    max_depth=5,
                    learning_rate=0.01,
                    n_estimators=600,
                    subsample=0.5,
                    colsample_bytree=0.25,
                    objective='binary:logistic',
                    random_state=42
                ))
            ])
            
            # Train the pipeline on the training data
            imb_pipeline.fit(X_train_scaled, y_train)
            
            # Make predictions on the test set
            y_pred = imb_pipeline.predict(X_test_scaled)
            
            # Create the SHAP explainer
            final_estimator = imb_pipeline.named_steps['xgbclassifier']
            explainer = shap.Explainer(final_estimator, X_train_scaled)
            
            # Calculate SHAP values
            shap_values = explainer(X_test_scaled, check_additivity=False)
            
            # Calculate evaluation metrics
            balanced_recall = balanced_accuracy_score(y_test, y_pred)
            accuracy = accuracy_score(y_test, y_pred)
            
            return {
                "accuracy": accuracy,
                "balanced_recall": balanced_recall,
                "shap_values": shap_values,
                "X_test_scaled": X_test_scaled,
                "X_train": X_train,
                "X_train_scaled": X_train_scaled,
                "y_pred": y_pred,
                "y_test": y_test,
                "y_train": y_train,
                "X_test": X_test
        }
            </code></pre>
                <p>Here, I defined a function that takes a dataset and a target column name as inputs and trains an XGBoost classification model with SMOTE oversampling.</p> 
                <p>The function preprocesses the data by dropping missing values, converting the target into binary classes, and splitting it into training and testing sets. 
                <p>It standardizes features and builds a pipeline with SMOTE and XGBoost classifier components. The model is trained, predictions are made, and SHAP values are computed so that we can examine feature importance.</p>
            <div class="centered-section">
                <button class="link-button" onclick="window.open('https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/', '_blank')">What is SMOTE?</button>
            </div>
            </section>

            <section>
                <h2>Above the Mendoza Line: Testing wBOA</h2>
                <p>So here we go! After cleaning the data and setting things up for success, I ran the model. I chose wOBA as my first model, because it mesaures overall hitting production.</p>
                <p>I called the function and ran it.</p>
                <pre><code class="language-python">
                result_woba = train_and_evaluate_classifacation_model(batter_woba_df, 'zscore_difference_woba')

                    Accuracy: 0.65
                    Recall: 0.65    
                </code></pre>
                <p>In all seriousness, it wasn't perfect, but I was actually happy enough with it considering all the things that could go wrong or right for a batter.</p>
                <p>Now, what I was most interested in is the feature proformance. What elements were the algorithms most using to make this perdiction. Because there are so many columns, the feature importanc is relativeley low even for the most important.</p>
                <p>I used the SHAP module to calculate and plot the imprtance.</p>
                <div style="text-align: center;">
                    <p><strong>SHAP Violin Plot</strong></p>
                <img src="static/images/shap_summary_plot_woba copy.png" alt="Shap wOBA">
                </div>
                <p>Turns out, to predict over hitting preformance, you can use past .... to predict it..</p>
            </section>

            <section>
                <h2>Handedness</h2>
                <p>Whe going over the data, I doubled checked to see if BABIP has predictably increased for left-handed batters as this is the group most affected by defensive shifts. And, yes. Almost all the left-handers BABIP Z-Score Difference was positive.</p>
                <p>I figured adding handedness in my models would massiveley increase my accuracy. So much in fact, that I considered not doing it because I was going to have to do some ETL to get to it.</p>
                <p>And I thought, handedness might be closely linked with the outcome variable, to the extent that it might introduce redundancy or overfitting in a predictive model. But, I did it anyway so I could see some models in the 90s.</p>
                <p>Fangraphs does not include handedness, perhaps becuase they do include splits batting AVG, OPS, Etc. agaist RHP v LHP(more on that later). So I had to find a database with the Handedness included, which I did at <a href="https://baseballsavant.mlb.com//">Baseball Savant.</a></p>
                <p>After an annoying merge and one HAWWWT coding, I ran my models and they all turned out worse by at least two percentage points, except the one we might expect BABIP.</p>
                <br>
                <p style="text-align: center;"><strong>BABIP Model Performance without Handedness</strong></p>
                <pre><code class="language-python">
        result_babip = train_and_evaluate_classifacation_model(batter_babip_df, 'zscore_difference_babip')

                Accuracy: 0.73
                Recall: 0.73
                </code></pre>
                <p style="text-align: center;"><strong>BABIP Model Performance with Handedness</strong></p>
                <pre><code class="language-python">
    result_babip_w_bat_side = train_and_evaluate_classifacation_model(batter_w_bat_side, 'zscore_difference_babip')

                Accuracy: 0.75
                Recall: 0.75
                </code></pre>
                <p>However, for the rest of the models, the performance dip a point or two. Because handedness seemed such a stark distinction, I thought it would really help my model.</p>
                <p>I think two elements effected this.</p>
                <p>First, because I included so many feature, the model mostly likely handeled handedness inevertley through splits and other features. Adding bat-side, was most likley redudant and may have contadicted all elements that helped make the pervious predictions.</p>
                <p>Second, as outlined by the article below, the shift may have had a little less to do with performance than orginally predicted.</p>
                <div class="centered-section">
                    <button class="link-button" onclick="window.open('https://www.mlb.com/news/shift-rule-effect-on-lefty-hitters/', '_blank')">Will Shift Ban Help These 6 Hitters? Not So Fast...</button>
                </div>
                </section> 
            </section>

            <section>
                <h2>Barrel Up: Results</h2>
                <div class="image-container">
                    <img src="images/lees.png" alt="Lee's Restaurant">
                </div>
                <p>Best model was SLG. 77% accurancy.</p>
                <p>Tough to predict so pretty happy. </p>
                <p>Shap and feature importance for each one.</p>
                <p>I made a player dashboard where we can imput a players name and it reurns the zscore differences for all the target variables.</p>
                <p>And, more importantly whether we got the prediction right.</p>
                <button class="dash-button" onclick="window.open('http://127.0.0.1:8050', '_blank')">Prediction Dashboard</button>
            </section>

            <section>
                <h2>Rounding Third: Where to Go from Here</h2>
                <p>Obviously, home right. No seriously, let's talk about the usefulness of this project.</p>
                <p>First, I'd like to talk about Tim Anderson of my hometown Chicago White Sox.</p>
                <p>He's had a horrible year. But</p>
                <p> Our queries found that Alice Fong Young's School did the best and the marked on our story map.<p>
                <p>In case you are wondering, Sts. Peter and Paul School did the worst.</p>
            </section>

</body>
</html>